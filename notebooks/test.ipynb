{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.losses import Huber\n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import MinMaxScaler as Ms\n",
    "\n",
    "def build_model(input_shape, num_stocks=42):\n",
    "    \"\"\"\n",
    "    Builds and compiles an LSTM model.\n",
    "\n",
    "    Parameters:\n",
    "    input_shape (tuple): The shape of the input data (time_steps, num_features).\n",
    "    num_stocks (int): The number of stocks (output dimensions).\n",
    "\n",
    "    Returns:\n",
    "    keras.models.Sequential: Compiled LSTM model.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential([\n",
    "        LSTM(50, return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(0.2),\n",
    "        LSTM(50),\n",
    "        Dropout(0.2),\n",
    "        Dense(num_stocks, activation='tanh')  # Using tanh activation\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=RMSprop(), loss=Huber())  # Using RMSprop and Huber Loss\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "def model_atmpt_2(input_shape, num_stocks=5):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50, return_sequences=True, input_shape = input_shape))\n",
    "\n",
    "    model.add(Dropout(0.1)) \n",
    "    model.add(LSTM(units=50))\n",
    "\n",
    "    model.add(Dense(42))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "def train_model(model, X, y, epochs=20, batch_size=64, validation_split=0.2):\n",
    "    \"\"\"\n",
    "    Trains the LSTM model.\n",
    "\n",
    "    Parameters:\n",
    "    model (keras.models.Sequential): The compiled LSTM model.\n",
    "    X (numpy.ndarray): Input features.\n",
    "    y (numpy.ndarray): Target values.\n",
    "    epochs (int): Number of training epochs.\n",
    "    batch_size (int): Batch size for training.\n",
    "    validation_split (float): Fraction of data to use for validation.\n",
    "\n",
    "    Returns:\n",
    "    keras.callbacks.History: History object containing training history.\n",
    "    \"\"\"\n",
    "    \n",
    "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
    "    return history\n",
    "\n",
    "\n",
    "def implement_model(df, model, train_seq, train_label, test_seq, test_label, epochs=3, batch_size=64, verbose=1):\n",
    "    \"\"\"\n",
    "    Trains the LSTM model.\n",
    "\n",
    "    Parameters:\n",
    "    model (keras.models.Sequential): The compiled LSTM model.\n",
    "    train_seq (numpy.ndarray): Input features.\n",
    "    train_label (numpy.ndarray): Target values.\n",
    "    validataion_data : tuple of test seq and label\n",
    "    epochs (int): Number of training epochs.\n",
    "    batch_size (int): Batch size for training.\n",
    "\n",
    "    Returns:\n",
    "    keras.callbacks.History: History object containing training history.\n",
    "    \"\"\"\n",
    "    model.fit(train_seq, train_label, epochs=epochs, batch_size=batch_size, validation_data=(test_seq, test_label))\n",
    "    return model\n",
    "\n",
    "def print_df(df, filename):\n",
    "   with open(f\"{filename}.txt\", \"w\") as f:\n",
    "        f.write(df.head(50).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning, normalizing, feature engineering, scaling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def merge_data(stock_data, gov_data):\n",
    "    \"\"\"\n",
    "    Merges stock data and government spending data on the date for each ticker.\n",
    "    \n",
    "    Parameters:\n",
    "    stock_data (dict): A dictionary with ticker symbols as keys and their corresponding DataFrames as values.\n",
    "    gov_data (pd.DataFrame): A DataFrame containing the government spending data.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: A merged DataFrame with stock data and government spending data.\n",
    "    \"\"\"\n",
    "    print(\"Stock Data Columns:\", stock_data.keys())\n",
    "    print(\"USASpending Data Columns:\", gov_data.columns.tolist())\n",
    "    \n",
    "    # Ensure the Date column in government spending data is datetime\n",
    "    gov_data['Date'] = pd.to_datetime(gov_data['Date'])\n",
    "\n",
    "\n",
    "    merged_data = pd.DataFrame()\n",
    "    for ticker, df in stock_data.items():\n",
    "        df.reset_index(inplace=True)  # Ensure the date is a column\n",
    "        df.rename(columns={'t': 'Date', 'o': 'Open', 'h': 'High', 'l': 'Low', 'c': 'Close', 'v': 'Volume'}, inplace=True)\n",
    "        df['Ticker'] = ticker\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        merged = pd.merge(df, gov_data, on='Date', how='inner')  # Merge on the Date column\n",
    "        merged_data = pd.concat([merged_data, merged], ignore_index=True)\n",
    "    return merged_data\n",
    "\n",
    "def split_train_test(df):\n",
    "    Ms = MinMaxScaler()\n",
    "    df[df.columns] = Ms.fit_transform(df)\n",
    "\n",
    "    training_size= round(len(df)*0.80)\n",
    "    train_data= df[:training_size]\n",
    "    test_data=df[training_size:]\n",
    "    return train_data, test_data\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "def create_sequences(df, window_size=30, column_A=\"Date\"):\n",
    "    \"\"\"\n",
    "    Prepares data for LSTM by reshaping it into a 3D array.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing\n",
    "    window_size (int): The number of lagged time steps.\n",
    "    \n",
    "    Returns:\n",
    "    np.array, np.array: The reshaped features and targets.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    strt_idx = 0\n",
    "    for stp_idx in range(window_size, len(df)):\n",
    "        sequences.append(df.iloc[strt_idx:stp_idx].values)\n",
    "        labels.append(df.iloc[stp_idx].values)\n",
    "        strt_idx+=1\n",
    "    return(np.array(sequences), np.array(labels))\n",
    "    \n",
    "\n",
    "\n",
    "    #previous thought which was close but not complete\n",
    "    print(df)\n",
    "    values = df.values#drop(columns=[column_A]).values\n",
    "    X, y = [], []\n",
    "    for i in range(window_size, len(values)):\n",
    "        X.append(values[i-window_size:i, :-1])\n",
    "        y.append(values[i, -1])\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for retrieving and processing Polygon.io data\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "POLYGON_API_KEY='q6YjvzTWAp_OkhFvfxwfgrtIVOpddl_V'\n",
    "POLYGON_API_URL='https://api.polygon.io'\n",
    "\n",
    "def get_historical_stock_data(ticker, start_date, end_date, POLYGON_API_KEY='q6YjvzTWAp_OkhFvfxwfgrtIVOpddl_V', POLYGON_API_URL='https://api.polygon.io'):\n",
    "    \"\"\"\n",
    "    Fetches historical stock data for a given ticker from Polygon.io.\n",
    "\n",
    "    Parameters:\n",
    "    ticker (str): The stock ticker symbol to fetch data for.\n",
    "    start_date (str): The start date for fetching data in 'YYYY-MM-DD' format.\n",
    "    end_date (str): The end date for fetching data in 'YYYY-MM-DD' format.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing OHLCV data for the ticker.\n",
    "    \"\"\"\n",
    "    \n",
    "    url = f\"{POLYGON_API_URL}/v2/aggs/ticker/{ticker}/range/1/day/{start_date}/{end_date}?adjusted=true&sort=asc&limit=5000&apiKey={POLYGON_API_KEY}\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        data = response.json()\n",
    "\n",
    "        data = response.json()\n",
    "        if 'results' in data and data['results']:\n",
    "            df = pd.DataFrame(data['results'])\n",
    "            # print(df.t[3])\n",
    "            df['t_a'] = pd.to_datetime(df['t'], unit =\"ms\", yearfirst=True)\n",
    "            df[\"t\"] =  df['t_a'].dt.date\n",
    "            \n",
    "            \n",
    "            df.set_index('t', inplace=True)\n",
    "            # print(type(df.index))\n",
    "            df.rename(columns={'o': f'o_{ticker}', 'h': f'h_{ticker}', 'l': f'l_{ticker}', 'c': f'c_{ticker}','v':f'v_{ticker}'}, inplace=True)\n",
    "            # print(df.head())\n",
    "            \n",
    "          \n",
    "            df[f'{ticker}_SMA_10'] = df[f'c_{ticker}'].rolling(window=10).mean()\n",
    "            df[f'{ticker}_SMA_50'] = df[f'c_{ticker}'].rolling(window=50).mean()\n",
    "            df[f'{ticker}_Returns'] = df[f'c_{ticker}'].pct_change()\n",
    "            df.dropna(inplace=True)\n",
    "            \n",
    "            return df[[f'o_{ticker}', f'h_{ticker}', f'l_{ticker}', f'c_{ticker}',f'v_{ticker}', f'{ticker}_SMA_10',f'{ticker}_SMA_50',f'{ticker}_Returns']]#df[['o', 'h', 'l', 'c', 'v']]\n",
    "        else:\n",
    "            print(f\"No data available for {ticker} in the specified date range.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    except ValueError as e:\n",
    "        print(f\"JSON decode error for {ticker}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def get_data_for_multiple_tickers(tickers=['NGL', 'TSLA', 'AAPL', 'V', 'NSRGY'], start_date= '2023-10-01', end_date = '2024-12-30'):\n",
    "    \"\"\"\n",
    "    Fetches historical stock data for multiple tickers from Polygon.io.\n",
    "\n",
    "    Parameters:\n",
    "    tickers (list): A list of stock ticker symbols to fetch data for.\n",
    "    start_date (str): The start date for fetching data in 'YYYY-MM-DD' format.\n",
    "    end_date (str): The end date for fetching data in 'YYYY-MM-DD' format.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with ticker symbols as keys and their corresponding DataFrames as values.\n",
    "    \"\"\"\n",
    "    \n",
    "    stock_data = {}\n",
    "    for ticker in tickers:\n",
    "        data = get_historical_stock_data(ticker, start_date, end_date)\n",
    "        print(f\"Fetched data for {ticker}\")\n",
    "        time.sleep(1)\n",
    "        if not data.empty:\n",
    "            stock_data[ticker] = data\n",
    "    return stock_data\n",
    "\n",
    "def merge_dataframes(starting_df, dict_stock_dfs):\n",
    "    \"\"\" merges the dataframes\"\"\"\n",
    "    merged_data=starting_df\n",
    "        #identifies the starting dataframe for subsequent merges\n",
    "    for data_frame in dict_stock_dfs:\n",
    "        #will iterate through the keys of the dictionaries of stock dataframes, these keys will be the tickers of the stocks\n",
    "        df_to_add = dict_stock_dfs[data_frame]\n",
    "            #accesses the current dataframe in the dictionary of stock dataframes  \n",
    "        merged_data = pd.merge(merged_data, df_to_add, right_index=True, left_index=True)\n",
    "            #merges the usa spending for each date with the corresponding stock data for that date\n",
    "            #since the dates are the indicies, the merge occurs on the indicies\n",
    "            #each stock dataframe has to have column titles that are unique to it's stock so that the stocks can all be in the same dataframe without overwriting eachothers data\n",
    "                # ie every stock dataframe has data for o h l c and v so we add the stock ticker to the column name as an extra identifier\n",
    "    merged_data.rename_axis(\"Date\", inplace=True)\n",
    "        #retains the original index identifier so that the index can be accessed using the keyword \"Date\" in future code\n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_usaspending_data(filepath='/data/users/mccallke0364/Algorithmic Trading/github/CS-491-Algorithmic-Trading-Project/src/data_collection/usaspending_data.csv'):\n",
    "    \"\"\"\n",
    "    Loads government spending data from a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    filepath (str): The path to the CSV file.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing the government spending data.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath, parse_dates=['Date'], header=0, index_col=0)\n",
    "    df.index = pd.to_datetime(df.index, unit='ms')\n",
    "    print_df(df.sort_index(), \"usa_spend\")\n",
    " \n",
    "    # df.set_index([\"Date\"])\n",
    "    # print(type(df.index))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched data for NGL\n",
      "Fetched data for TSLA\n",
      "Fetched data for AAPL\n",
      "Fetched data for V\n",
      "Fetched data for NSRGY\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 30, 50)            18600     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 30, 50)            0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 50)                20200     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 42)                2142      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40942 (159.93 KB)\n",
      "Trainable params: 40942 (159.93 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from data_collection.polygon_data import *\n",
    "# #from data_collection.bezinga_data import get_government_trades_data\n",
    "# from data_collection.usaspending_data import *\n",
    "# from preprocessing.preprocess_data import *\n",
    "# from model.lstm_model import *\n",
    "# from model.utils import *\n",
    "\n",
    "\n",
    "################---------Collect data from polygon.io and usa spending---------################\n",
    "dict_stock_dfs = get_data_for_multiple_tickers() \n",
    "    # a dictionary of each stocks dataframe \n",
    "    # set up with the default stocks of ['NGL', 'TSLA', 'AAPL', 'V', 'NSRGY'] and dates start_date = '2023-10-01' end_date = '2024-12-30' \n",
    "usa_spending_data = get_usaspending_data() \n",
    "    # dataframe of the usa spending data for each date \n",
    "    # set up with the default path to be 'data_collection/usaspending_data.csv'\n",
    "usa_spending_data = usa_spending_data.filter([\"total_obligations\",  \"total_outlayed_amount\"])\n",
    "full_dataframe = merge_dataframes(usa_spending_data, dict_stock_dfs)\n",
    "    #merges all the stock dataframes and usa spending based on the date the data was collected\n",
    "\n",
    "print_df(full_dataframe, \"full_df\")\n",
    "\n",
    "################---------Process data for model---------################\n",
    "train_data, test_data = split_train_test(full_dataframe)\n",
    "    #split data 80:20 for training and testing\n",
    "train_seq, train_label = create_sequences(train_data)\n",
    "    #creating the sequence for training\n",
    "test_seq, test_label = create_sequences(test_data)\n",
    "    #creating the sequence for testing\n",
    "\n",
    "\n",
    "# print(train_seq)\n",
    "################---------Build model---------################\n",
    "# model = model_atmpt_2((train_seq.shape[1], train_seq.shape[2]))\n",
    "    #building model, automatically takes the default number of stocks, (5), since it is not specified in the call\n",
    "model = build_model((train_seq.shape[1], train_seq.shape[2]))\n",
    "\n",
    "\n",
    "# print_df(implement_model(full_dataframe, model, train_seq, train_label, test_seq, test_label), \"full_model_1\")\n",
    "    #used train_model_pre_split since the data has already been split into training and testing data\n",
    "    #anything not specified has a default in the function call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = model_atmpt_2((train_seq.shape[1], train_seq.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "8485/8485 [==============================] - 203s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/3\n",
      "8485/8485 [==============================] - 205s 24ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/3\n",
      "8485/8485 [==============================] - 205s 24ms/step - loss: nan - val_loss: nan\n",
      "4242/4242 [==============================] - 19s 4ms/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Ms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-c371f7e2a924>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimplement_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_dataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"implementdf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-8a322c6c7450>\u001b[0m in \u001b[0;36mimplement_model\u001b[0;34m(df, model, train_seq, train_label, test_seq, test_label, epochs, batch_size, verbose)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mtest_p_df\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_predicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mprint_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_p_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test_predict_a\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0mtest_inverse_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_predicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0mtest_i_p_df\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_inverse_predicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mprint_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_i_p_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"inverse_predict_a\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Ms' is not defined"
     ]
    }
   ],
   "source": [
    "model_frame = implement_model(full_dataframe, model, train_seq, train_label, test_seq, test_label)\n",
    "print_df(model_frame, \"implementdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted = model.predict(test_seq)\n",
    "test_p_df= pd.DataFrame(test_predicted)\n",
    "print_df(test_p_df, \"test_predict_a\")\n",
    "test_inverse_predicted = Ms.inverse_transform(test_predicted)\n",
    "test_i_p_df= pd.DataFrame(test_inverse_predicted)\n",
    "print_df(test_i_p_df, \"inverse_predict_a\")\n",
    "new_df = pd.concat(df, test_p_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
